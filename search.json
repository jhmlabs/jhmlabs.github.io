[
  {
    "objectID": "posts/2024-12-19-PneqNP_Update/index.html",
    "href": "posts/2024-12-19-PneqNP_Update/index.html",
    "title": "On C.A. Feinstein’s Elegant Argument that P neq NP",
    "section": "",
    "text": "We first read Craig Alan Feinstein’s paper (Feinstein 2011) in 2020. His paper was persuasive and indeed elegant, yet we understood that his argument was not completely formal and not entirely mathematical. The critics were somewhat fair in their stubborn dismissal of his argument. Thus we took it upon ourselves to formalize and provide the mathematical foundations for his argument. This work is contained in our preprint (Martel 2024) available here: (On C.A. Feinstein’s Elegant Argument that P neq NP)[www.github.com/jhmartel/PneqNP/].\nThe argument is summarized as follows:\n\nDeciding supports of Borel-Radon measures is the Universal Decision Problem in mathematics.\nDeterministic algorithms are exactly explicit definitions. Nothing more and nothing less.\n“Guess and Check” is the Universal Deterministic Algorithm. “How do you find a needle in a haystack? What colour am I thinking of?”\nComputational Complexity and Entropy are the same Quantity.\nTao-Szemeredi Regularity Lemma (Tao 2005) implies every Deterministic Algorithm is Conditional Guess and Check (“Look here and not there”).\nZero Subset Sums is a maximally unstructured decision problem according to Tao-Szemeredi Structure Lemma.\nSolving Zero Subset Sums by “guess and check” over exponentially many subsets is the only deterministic algorithm to decide Zero Subset Sums, and therefore \\(P \\neq NP\\) (Cook 2000).\n\nThese seven propositions contain the crux of C.A. Feinstein’s elegant argument.\n\n\n\n\nReferences\n\nCook, Stephen. 2000. “The p Versus NP Problem.” Clay Mathematics Institute 2: 6.\n\n\nFeinstein, Craig Alan. 2011. “An Elegant Argument That \\(P \\neq NP\\).” Progress in Physics 2: 30–31.\n\n\nMartel, Justin Harry. 2024. “On c.a.feinstein’s Elegant Argument That p Neq NP.” www.github.com/jhmartel/PneqNP/.\n\n\nTao, Terence. 2005. “Szemeredi’s Regularity Lemma Revisited.” arXiv Preprint Math/0504472."
  },
  {
    "objectID": "posts/2024-06-07-VoronoiModuliSpaces/index.html",
    "href": "posts/2024-06-07-VoronoiModuliSpaces/index.html",
    "title": "On Voronoi’s Projective Model of Hyperbolic Geometry",
    "section": "",
    "text": "Hyperbolic geometry was something hidden from mankind for thousands of years. Mankind looked and saw the earth was flat and the heavens were spherical. Euclidean geometry developed according to straight lines, parallels, right angles, and Pythagorean \\(a^2+b^2=c^2.\\) The heavenly spheres were traversed by great arcs, intersections of flat planes with large spheres. The sum of angles in a geodesic triangle satisfied \\(\\alpha+\\beta+\\gamma = \\pi\\) assuming that doubly infinite parallels could be drawn through arbitrary points. This hypothesis, key to Euclidean geometry, was Euclid’s Fifth Postulate. The Euclidean geometry developed by axioms, by logical deductions and constructions, and it remained open question whether the Fifth Postulate was a consequence of the simpler Euclidean postulates about points and lines. Men were driven to madness searching out ingenious proofs – all ultimately incorrect and flawed – to derive the Fifth Postulate from basic premises of point line geometry.\nThe possibility of hyperbolic geometry was hidden from mankind for thousands of years. Hidden until various models of hyperbolic geometry and hyperbolic planes were described by Bolyai, Lobachevsky, Gauss, and later elaborated by Klein, Minkowski, Poincare, etc.. But ultimately, all these various models can be understood in a unified sense as projectivizations of a canonical model. This canonical model is Voronoi’s space of quadratic states which we describe below."
  },
  {
    "objectID": "posts/2024-06-07-VoronoiModuliSpaces/index.html#euclids-fifth-postulate-and-hidden-hyperbolic-geometry",
    "href": "posts/2024-06-07-VoronoiModuliSpaces/index.html#euclids-fifth-postulate-and-hidden-hyperbolic-geometry",
    "title": "On Voronoi’s Projective Model of Hyperbolic Geometry",
    "section": "",
    "text": "Hyperbolic geometry was something hidden from mankind for thousands of years. Mankind looked and saw the earth was flat and the heavens were spherical. Euclidean geometry developed according to straight lines, parallels, right angles, and Pythagorean \\(a^2+b^2=c^2.\\) The heavenly spheres were traversed by great arcs, intersections of flat planes with large spheres. The sum of angles in a geodesic triangle satisfied \\(\\alpha+\\beta+\\gamma = \\pi\\) assuming that doubly infinite parallels could be drawn through arbitrary points. This hypothesis, key to Euclidean geometry, was Euclid’s Fifth Postulate. The Euclidean geometry developed by axioms, by logical deductions and constructions, and it remained open question whether the Fifth Postulate was a consequence of the simpler Euclidean postulates about points and lines. Men were driven to madness searching out ingenious proofs – all ultimately incorrect and flawed – to derive the Fifth Postulate from basic premises of point line geometry.\nThe possibility of hyperbolic geometry was hidden from mankind for thousands of years. Hidden until various models of hyperbolic geometry and hyperbolic planes were described by Bolyai, Lobachevsky, Gauss, and later elaborated by Klein, Minkowski, Poincare, etc.. But ultimately, all these various models can be understood in a unified sense as projectivizations of a canonical model. This canonical model is Voronoi’s space of quadratic states which we describe below."
  },
  {
    "objectID": "posts/2024-06-07-VoronoiModuliSpaces/index.html#voronois-projective-cone",
    "href": "posts/2024-06-07-VoronoiModuliSpaces/index.html#voronois-projective-cone",
    "title": "On Voronoi’s Projective Model of Hyperbolic Geometry",
    "section": "Voronoi’s Projective Cone",
    "text": "Voronoi’s Projective Cone\nWe begin with a question.\nLet \\(D=D^2\\) be the two-dimensional unit disk \\(D=\\{z| |z| \\leq 1\\} \\subset {\\bf{R}}^2\\). Can the reader readily construct continuous group actions \\[(D, \\partial D) \\times GL({\\bf{R}}^2) \\to (D, \\partial D)\\] which satisfies the following properties:\n\nthe action is transitive on the interior;\npoint stabilizers are compact;\nthe action is elementary.\n\nThe final criteria (iii) is subjective, but important for calculations.\nThe reader has likely seen actions satisfying the first condition before, namely the group of “homographic mappings”, also known as Mobius transformations and their formulae. The action is conventionally presented in complex coordinates as \\[{\\bf{C}} \\times GL({\\bf{R}}^2)  \\to {\\bf{C}}, \\quad (z, \\begin{pmatrix}a & b \\\\ c & d \\end{pmatrix}) \\mapsto (az+b) / (cz+d),\\] after we identify \\({\\bf{R}}^2 \\approx \\bf{C}\\). But does anybody really understand the above formula? Is the formula for Mobius transformations not incredible?\nJHM Labs has always found the Mobious formula \\(\\frac{az+b}{cz+d}\\) hard to understand. Indeed we argue that it’s not elementary in the sense of (iii). Is it elementary that \\({\\bf{R}}^2\\) possesses a division operation \\((u,v)\\mapsto u/v\\)? Thus we argue that Mobius formulas are not elementary.\nWe propose the following description of an elementary action of \\(GL({\\bf{R}}^2)\\) on the disk \\(D\\). The description arises from representation theory. It begins with:\n(a1) The matrix group \\(GL({\\bf{R}}^2)\\) acts on \\({\\bf{R}}^2\\) via matrix multiplication \\(\\rho:(v,g) \\mapsto v.g\\).\n(a2) The above representation has natural dual representation \\(\\rho^*\\) of \\(GL({\\bf{R}}^2)\\) acting on the vector space of linear functionals \\({{\\bf{R}}^2}^*\\), where we adopt left action \\(g.\\ell(v):=\\ell(g^{-1}(v))\\).\n(a3) The symmetric square of the dual representation \\(Sym^2(\\rho^*)\\) yields \\(GL({\\bf{R}}^2)\\) acting on the vector space of homogeneous symmetric quadratic polynomials on \\({\\bf{R}}^2\\).\n(a4) We restrict \\(Sym^2(\\rho^*)\\) to the convex cone \\(Q\\) of semidefinite quadratic states which satisfy \\(q(v) \\geq 0\\) for all \\(v \\in {\\bf{R}}^2\\). The cone \\(Q\\) is Voronoi’s cone of quadratic states.\n(a5) The affine action \\(Q  \\times GL({\\bf{R}}^2)\\to Q\\) commutes with the scalar \\({\\bf{R}}^{\\times}_{&gt;0}\\)-action. Thus \\(GL({\\bf{R}}^2)\\) acts continuously on the projectivization \\(Proj(Q):=(Q-0) / {\\bf{R}}^{\\times}_{&gt;0}\\). But the projectivization of a cone is an affine bounded topological disk. Thus we find \\(PGL({\\bf{R}}^2)\\) acts nontrivially on the bounded disk.\nOne finds the criteria (i), (ii), and (iii) are satisfied for every representation in steps (a1)–(a5). Thus we\nClaim: The natural action of \\(GL({\\bf{R}}^2)\\) on the projectivized Voronoi cone \\(Proj(Q)\\) is an elementary group action satisfying criteria (i), (ii), (iii) above.\nHistorically the diverse models of hyperbolic geometries discovered by Gauss, Bolyai, Lobachevskii, Beltrami, Poincare, Klein, Minkowski, etc.. are all unified with Voronoi’s projective model. Indeed we observe that all the historical models are noncanonical sections of the projectivization \\(Q - 0 \\to Proj(Q)\\). For example, the hypersurfaces \\(\\{\\text{tr}=1\\}\\) and \\(\\{\\text{ disc }=1\\}\\) yield the Klein and Minkowski models of the hyperbolic plane. Compare (Thurston 2014), (Thurston 2022, 27:Ch.2). We remark that trace and dicriminant are defined as tensors on quadratic states.\nThis idea about Voronoi’s projective model has applications to Teichmueller spaces, and to arithmetic groups like \\(Sp({\\bf{R}}^{2g})\\) and \\(\\Gamma=Sp({\\bf{Z}}^{2g})\\). From Voronoi we learn the idea that there can exist many “equivalent” models of a given moduli space, but they are different sections of a universal projective model.\nFor example, the symmetric space associated to the group \\(Sp({\\bf{R}}^{2g})\\) is well-known as Siegel’s upper half space \\({\\bf{H}}_{2g}\\) consisting of complex \\(g \\times g\\) matrices \\(Z\\) which satisfy \\(Im(Z)&gt;0\\). This latter condition implies the Gaussian \\(e^-Z\\) is absolutely convergent. Compare Folland’s treatment (Folland 1989).\nFor Teichmueller space, the existence of a universal projective model is an open problem. Geometers are not easily moved past the Teichmueller metric, which strikes us as simple an arbitrary canonical section of a projectivization not yet understood.\n[To be continued - JHM]"
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html",
    "href": "posts/2024-05-24-PhysicsCalling/index.html",
    "title": "Physics is a Calling",
    "section": "",
    "text": "Our critical review of the foundations of special relativity is available here. The article has been completed for a year, but has not been distributed nor widely read. However it serves as a foundation stone for all our subsequent ideas."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#foundations-of-special-relativity-a-critical-review",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#foundations-of-special-relativity-a-critical-review",
    "title": "Physics is a Calling",
    "section": "",
    "text": "Our critical review of the foundations of special relativity is available here. The article has been completed for a year, but has not been distributed nor widely read. However it serves as a foundation stone for all our subsequent ideas."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-hamilton-and-gibbs-liouville",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-hamilton-and-gibbs-liouville",
    "title": "Physics is a Calling",
    "section": "Weber Hamilton and Gibbs Liouville",
    "text": "Weber Hamilton and Gibbs Liouville\nDraft available here. Work in progress."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-and-steam",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-and-steam",
    "title": "Physics is a Calling",
    "section": "Weber and Steam",
    "text": "Weber and Steam\nWeber has a physical kinematic description of the various states of water (ice, liquid, steam) in his final eighth memoir (Weber 1894). Most interesting is Weber’s hypothesis that steam is essentially water molecules with negatively charged satellites, and these negative satellites mutually repel one another, thereby making steam expansive. The hypothesis suggests that liquid water could be evaporated into steam somewhat more rapidly by applying an electric constant uniform electric field throughout the liquid."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-and-brehmstrallung",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-and-brehmstrallung",
    "title": "Physics is a Calling",
    "section": "Weber and Brehmstrallung",
    "text": "Weber and Brehmstrallung\nThe physics of W. Rontgen’s brehmstrallung “braking radiation” is simple and interesting. When the accelerated electron impinges and strikes the tungsten anode, then Rontgen argues that the tungsten anode absorbs the electron into the material tungsten lattice, causing a high frequency vibration within the anode, and also causing the anode to relax and radiate energy via photons with energies computed according to Planck-Einstein’s formula \\(\\delta E = h \\nu\\). But the question is whether the energy is radiated locally via photons or whether the energy is transmitted by action at a distance according to Weber’s electrodynamics."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#frequency-cycles-time",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#frequency-cycles-time",
    "title": "Physics is a Calling",
    "section": "Frequency, Cycles, Time",
    "text": "Frequency, Cycles, Time\nReviewing the quantum theory and Brehmstrallung, we begin to appreciate the influence of Einstein’s formula \\(E=h\\nu\\) where \\(\\nu\\) is the supposed frequency of the photon. Frequency \\(\\nu\\) is measured in units of cycles per second. Therefore frequency is a relative measure of cycles per “cycle”, as per the cycle which defines the unit of second. As per our earlier posts, time is not a proper unit in itself, but is strictly observed by matter in motion.\nAndre suggests that the a cycle is represented best as periodicity in the energy variables. For example, in the pendulum, it’s not the physical motion but the periodicity in the potential or kinetic energy variables. This is useful in 2-body systems with precession, where the precise orbits are not periodic but precess, although there is a obvious periodicity in the energy variables."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#photoelectric-effect",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#photoelectric-effect",
    "title": "Physics is a Calling",
    "section": "Photoelectric Effect",
    "text": "Photoelectric Effect\nThe discovery of the photoelectric effect by P. Lenard (Wheaton 1978) was also unexplained by the Maxwell field theory. Lenard observed that electrons were dislodged only when light exceeded a certain frequency (i.e. uv microwaves) independant of the light intensity or duration of exposure. Lenard observed that the more intense the light (higher frequency, and higher energy) then more electrons would be released from the plate, but increasing the frequency did not increase the velocity of the escaping electrons. Moreover a low frequency beam of light at high intensity does not “build up” or accumulate the energy required to produce photo-electrons. For Lenard, that the electron velocity is independant of the light intensity suggests that the energy for the electrons comes from the atom and not from the light. This was called Lenard’s “trigger” theory.\nA. Einstein introduced the photon heuristic (EINSTEIN 1905) in 1905 to account for Lenard’s observation. With Einstein’s photon hypothesis, light radiation becomes a localized packet of energy which is presumably travelling at the speed of light, and having a certain characteristic frequency \\(\\nu\\). In the above paper Einstein introduces the Planck-Einstein relation \\(\\Delta E = h\\nu\\) to account for the energy released by photoelectrons.\nThe Weber-Sansbury interpretation of the photoelectric effect is something similar to a PNP transistor. We imagine the source of light as a radio dipole source. The light frequency of the source is, in our model, realized as an oscillating electrodynamic dipole system. This oscillating system is causing an instantaneous action at a distance on the target system, i.e. the metallic photocell. The oscillating source induces a cumulative oscillation in the target photocell. This oscillation “drops the potential” at the target allowing mobile electrons in the photocell to opportunistically escape. These escaping electrons are the so-called photoelectrons."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#ralph-sansbury",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#ralph-sansbury",
    "title": "Physics is a Calling",
    "section": "Ralph Sansbury",
    "text": "Ralph Sansbury\nA key influence in the research program at JHMLabs is the late American physicist R. Sansbury. Compare this JHMLabs post.\nRemark. Admittedly we at JHMLabs are exploring the Weber-Sansbury hypothesis, that light is not a photon nor a wave that travels through space, but rather the effect of cumulative instantaneous action at a distance. Sansbury makes a subtle distinction between the so-called “speed of light”, and the apparent time delay required for a source transmitter to induce a measurable signal in a target receiver. The perceived time delay is not the time required for light to travel, but is the time required for subatomic oscillations to accumulate and attenuate into a measurable signal. This is one of Sansbury’s fundamental hypotheses. Thus the “speed of light” cannot be used as a foundation stone for physics, since the concept of “speed” imports the problematic issue of “How does light travel distance over time?” Rather, Sansbury proposes that the fundamental measurable quantity is the “time delay” for a signal at a source transmitter to be detected at the target receiver."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-sansbury-and-positronium",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-sansbury-and-positronium",
    "title": "Physics is a Calling",
    "section": "Weber-Sansbury and Positronium",
    "text": "Weber-Sansbury and Positronium\nWe are tempted by the possibility of explaining the relatively short lifetimes of positronium. Theoretical calculations with Weber’s potential applied to the Weber-Bohr-Sansbury model of the electron and “positron” could yield comparisons with the relatively short lifetimes of para-positronium and ortho-positronium (approx. \\(1.25 \\times 10^{-10} [secs]\\) and \\(1.38 \\times 10^{-7} [secs]\\), respectively.)\nIn the Weberian model, the classical electron and classical positron are replaced with the molecules \\([-2,1]\\) and \\([-1, +2]\\). Therefore a two-body problem is replaced with a microscopically six-body system, but which is acting as a four-body system in a macroscopic sense, and then indeed a a two-body problem at a larger scale. [insert figure]\nHow difficult is it to create positronium in the laboratory? There is possibility of using a radioactive source that ejects positrons. Specifically, Carl Andersen discovered that alpha particles (helium nuclei) bombarding beryllium (atomic number 4 Be !) emitted positrons. Beryllium Be is rather little known atomic element, but now appears very interesting from the Weber viewpoint.\nAnnihalation of electrons and positrons is claimed to represent the most direct example of Einstein’s \\(E=mc^2\\), where the claim is that all the mass-energy (in the Einstein terms) of the electron and positron is radiated away by gamma rays [ref]. It’s claimed that \\(0.511 [kEv]= m_{e_c} c^2\\) of energy are released in the annihalation in agreement with Einstein’s formula. Naively one might consider why the particles \\([e]\\) and \\([p]\\) even annihalate at all? Why do they not form a stable planetary system? The idea exacerbates the classical Maxwellian confusion about the inexplicable stability of the hydrogen atom.\nHow does the Weber potential model the emission of positrons when beryllium is bombarded by helium nuclei?"
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-sansbury-versus-bohr-atom",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-sansbury-versus-bohr-atom",
    "title": "Physics is a Calling",
    "section": "Weber Sansbury versus Bohr Atom",
    "text": "Weber Sansbury versus Bohr Atom\nSansbury proposes the structured electron hypothesis. However Sansbury was not apparently aware of Weber’s electrodynamics, nor the possibility of the stable \\([-1, -1]=[-2]\\) molecule [ref]. Rather Sansbury strictly performed his calculations according to the classical Coulomb electrostatic potential. Among his key insights is his understanding the possibility of balanced radiating systems which have no net radiation into the environment, but do mutually exchange radiation between themselves. This leads Sansbury to the idea of metastable orbits which again admit no net radiation into the environment, but do have a balanced exchange of energy between themselves.\nSansbury argues that there is no clear measurable difference between average frequencies during light emitted or absorbed during radiation, and the Bohr concept of discontinuous transition frequencies. Thus Sansbury argues that there is no reason not to accept J.W. Nicholson’s earlier suggestion that radiation was produced by an oscillating, i.e. orbiting electron. Specifically by the average frequency of oscillating electrons during transitions between excited metastable orbits.\nThe structured electron hypothesis is that the electron \\([-1]\\) as classically understood is structured in the form \\([-2, +1]\\). “Deeper into the atom” we have higher frequency motions. Stability and metastability of outward orbits requires frequencies which are synchronized with the higher frequency internal orbits.\nWe quote from (Sansbury, n.d., Ch.7):\nAt this point, we simply say that we are currently working on this question. Indeed Helium might have a macroscopic two electron orbit, but the Weber-Sansbury model allows the possibility of Helium as an \\(N\\)-body system with \\(N\\) not necessarily equal to 3."
  },
  {
    "objectID": "posts/2024-05-24-PhysicsCalling/index.html#weber-synchotron",
    "href": "posts/2024-05-24-PhysicsCalling/index.html#weber-synchotron",
    "title": "Physics is a Calling",
    "section": "Weber Synchotron",
    "text": "Weber Synchotron\nWe are struck by the intrinsice connection between the discontinuous \\(01010101\\) signals, and the continous cycles and motions of a particle. The basis of a synchotron particle accelerator is a binary voltage \\(010101\\) which is maintained while the accelerated particle occupies the distinct halfspaces in the synchotron. There’s more to say…\n[-JHM]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jhm labs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOn C.A. Feinstein’s Elegant Argument that P neq NP\n\n\n\n\n\nOur proof of C.A.Feinstein’s Elegant Argument that P neq NP is equivalent to proving that Guess and Check is the only deterministic algorithm which can decide What Colour Am I Thinking Of?\n\n\n\n\n\nDec 13, 2024\n\n\nJHM\n\n\n\n\n\n\n\n\n\n\n\n\nOn the Efficient Computing Frontier\n\n\n\n\n\nThere is inherent ontological gap between representations and reality, between images of cats and cats that meow. This gap is independant of computational resources and represents the inherent entropy uncertainty in any given representation.\n\n\n\n\n\nDec 12, 2024\n\n\nJHM\n\n\n\n\n\n\n\n\n\n\n\n\nOn Voronoi’s Projective Model of Hyperbolic Geometry\n\n\n\n\n\nVoronoi rushed into the room exclaiming, It’s linear, It’s all linear!\n\n\n\n\n\nJun 7, 2024\n\n\nJHM\n\n\n\n\n\n\n\n\n\n\n\n\nIdea On Steklov-Selberg Trace Formula for Arithmetic Spaces\n\n\n\n\n\nWe have idea about the Steklov spectrum versus the Laplace spectrum on noncompact arithmetic spaces. This suggests a Steklov Selberg trace formula on rational bordifications. Thus we bypass all complications related to continuous versus discrete spectra of Laplacian on noncompact spaces, and immediately obtain discrete spectrum.\n\n\n\n\n\nJun 6, 2024\n\n\nJHM\n\n\n\n\n\n\n\n\n\n\n\n\nTao-Szemeredi, Significant Bits, and Under-Over Fitting\n\n\n\n\n\nWe introduce an idea about Tao-Szemeredi and it’s application to understanding under and over fitting in the training of neural nets. Basically there is a certain entropy dimension which supports the most significant coarse bits. These significant coarse bits are the proper goal of training, whereas the backpropagation gradient step tends to approach then blow past these significant coarse bits during optimization. Under- and over- fitting occurs according to our hypothesis depending on whether the neural net is supported below or above the dimension of the most significant bits. This allows, in our estimate, a priori estimates about the effectiveness of retraining and transferability of neural nets.\n\n\n\n\n\nMay 25, 2024\n\n\nJHM\n\n\n\n\n\n\n\n\n\n\n\n\nPhysics is a Calling\n\n\n\n\n\nThe basic premise at JHMLabs is that Wilhelm Weber’s and Ralph Sansbury’s work synthesizes into a completely unified Classical Physics 2.0 which demonstrates that Bohr’s quantum hypothesis is disposable, and the stability of the atom can be resolved according to classical principles. We wish we only had the opportunity to make our case to Einstein, Rutherford, Planck, Ampere, Weber, Gauss, the masters themselves. \n\n\n\n\n\nMay 24, 2024\n\n\nJHM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Dr. JHM is a Canadian mathematician, physicist, writer, musician. His interests include history of physics, W.E. Weber’s electrodynamics, algorithms and P neq NP, singularities in optimal transport, spines of the mapping class group of hyperbolic surfaces, sweepouts and Dold-Thom."
  },
  {
    "objectID": "about.html#selected-writings",
    "href": "about.html#selected-writings",
    "title": "About",
    "section": "Selected Writings",
    "text": "Selected Writings\n\n[A New Spine for Teichmueller’s Space of Closed Hyperbolic Surfaces (in progress)] (https://github.com/jhmartel/MCG/)\nFoundations of Special Relativity and Light Propagation in Vacuum: A Critical Review\nTopology of Singularities of Optimal Semicouplings"
  },
  {
    "objectID": "posts/2024-05-23-TaoSzemerediOverfitting/index.html",
    "href": "posts/2024-05-23-TaoSzemerediOverfitting/index.html",
    "title": "Tao-Szemeredi, Significant Bits, and Under-Over Fitting",
    "section": "",
    "text": "We want to introduce an idea illustrating Tao-Szemeredi Lemma. The idea is that the under/overfitting inflection point in supervised machine learning models can be understood as the dimension on which the most significant bits relating the input and output data. First we review the basics of neural nets."
  },
  {
    "objectID": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#back-propagation-gradient-step",
    "href": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#back-propagation-gradient-step",
    "title": "Tao-Szemeredi, Significant Bits, and Under-Over Fitting",
    "section": "Back Propagation Gradient Step",
    "text": "Back Propagation Gradient Step\nOur reference for Machine Learning is (Chollet 2021). It’s well known that “the fundamental issue in machine learning is the tension between optimization and generalization.” Here optimization refers to adjusting models to get the best performance possible on the training data (so called “learning” aspect of ML), and generalization refers to the performance of the trained model on new data inputs. The goal of ML is to obtain good generalized performance while training the model on limited training data.\nNeural networks (NN) are models consisting of various layers. Each layer is a transformation (nonlinear) of the input data. NN’s are compositions of various layers of the form \\(x\\mapsto relu(Wx+b)\\) and \\(x\\mapsto K.x\\). The parameters \\(W, b, K\\) are the weights of the layer. Training a NN consists in adjusting the weights such that the model is nearly optimal on training data. The training loop is iterated as many times as necessary. To paraphrase from (Chollet 2021, 2.4), the training loop consists of:\n\nDraw a batch of training samples \\(x\\) and corresponding targets \\(y\\).\nRun the model on \\(x\\) ( a step called forward pass) to obtain predictions \\(y_{pred}\\).\nCompute the loss of the model on the batch, i.e. measure the mismatch between \\(y\\) and \\(y_{pred}\\).\nUpdate all weights in the model which slightly reduces the loss on this batch.\nRepeat as necessary.\n\nStep 4. is the so-called backpropagation step. Using the differentiability of the loss function, we can abstractly realize Step 4. using backwards gradient flow. I.e. we differentiate the loss function with respect to the weights, and take a small negative gradient step to decrease the loss. In the literature this is called “stochastic gradient descent” since the batch drawn in Step 1. is basically random, and therefore the backwards gradient step is somewhat randomly chosen. In practice loss function is a composition of many relu layers and tensor operations, and therefore the gradient of the loss is computed symbolically via chain rule.\nThe backwards gradient step is somewhat undefined. Indeed the step size of the gradient flow is arbitrary, and likewise the number of gradient step iterations is underdetermined. A priori we do not know which step sizes to use, and how many iterations until the model becomes overfit."
  },
  {
    "objectID": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#under-over-fitting-entropy-and-the-inflection-point.",
    "href": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#under-over-fitting-entropy-and-the-inflection-point.",
    "title": "Tao-Szemeredi, Significant Bits, and Under-Over Fitting",
    "section": "Under Over Fitting, Entropy and the Inflection Point.",
    "text": "Under Over Fitting, Entropy and the Inflection Point.\nWhen the model is beginning to be trained, there is typically some apparent correlation between optimization and generalization. As the model begins to perform better on training data, the generalized performance on new data will also be improving. At this stage the model is considered “underfit”. There are still correlations in the training data which the model is learning, and which correlations are relevant to the generalized data. But after a certain amount of training (i.e. backpropagation gradient steps), the generalized performance stops improving and begins to decline and degrade. The model is “overfit” at this stage of training, and is understood to be learning patterns which are hyper specific to the training data and irrelevant to the new data.\nThe broader question which we are studying is seeing entropy as dimension. Given the problem of predicting correlations, we want to identify the dimension of the correlation support. Influential in our thinking is Tao-Szemeredi Regularity Lemma (Tao 2005). The basic problem of under/overfitting is that the gradient step back propagation disregards the dimension of the correlation, and the usual optimization step will approach and then “blow past” the dimension of the correlation. Our proposal is that the “inflection point” where underfitting becomes overfitting can be characterized as the dimension of the correlation.\nOur critique of backpropagation gradient step is that the variational principle is not strictly correct. Is the purpose of training to optimize the loss function on training data? Or is the purpose of training to identify the significant coarse bits which support the majority of the correlation? We argue that generalization requires the significant coarse bits to be identified, especially since we are interested in new inputs beyond the training batch. Indeed then the training should emphasize “performance loss” with respect to weights defined on the coarse parameters.\nThis raises the interesting question of deciding the dimension of various correlations. For example, probabilists can always argue via Borel Isomorphism Theorem (Hatko 2013) that the naive topological dimension of a probability measure is not invariantly defined, with every measure space being measure isomorphic to a subset of the unit interval. But the entropy of the measure space is invariantly defined, and related essentially to the minimal dimension by which the states of the measure are distinguished."
  },
  {
    "objectID": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#transferability-problem.",
    "href": "posts/2024-05-23-TaoSzemerediOverfitting/index.html#transferability-problem.",
    "title": "Tao-Szemeredi, Significant Bits, and Under-Over Fitting",
    "section": "Transferability Problem.",
    "text": "Transferability Problem.\nSpotify wants to predict user preferences. But users come and go, they drop in and drop out, and the user distribution can sharply change over time. But the GNN which are developed would like to be transferred onto new users. Is it necessary to retrain the GNN? This is unreasonable. But can we predict a priori the performance of these pretrained GNNs on new user distributions? What if there is a sharper change in preferences, at what point is it necessary to retrain the GNN?\nIn our approach, the transferrability problem and the underfitting/overfitting problem are examined from Tao-Szemeredi. The point of Szemeredi, in the Spotify setting, is that you don’t need an algorithm trained on the total fine user bits. There is a maximal number of coarse bits which correlate, and everything is independant of all further bits. For example, highly detailed fine information about the users is not relevant to Spotify preferences. There are only a few coarse bits which are relevant (age, gender, basics) to predicting \\(E\\) events, and such that \\(E\\) is independant of any further bits.\nTherefore we train models on given distributions, but look to also train and decide the maximally correlated coarse variables. The transfer to new distributions will then immediately know which coarse variables are significantly correlated.\nThe inflection point for underfitting/overfitting are the max entropy coarse bits. When the backpropagation (gradient step) is applied beyond the coarse bits, then the model is being forced to gradient step on bits which are independant of the events. This is where performance on generalized input will diminish. The model is training parameters based on irrelevant bits!\nWhat happens to GNNs when they are trained on inputs which are independant of their outputs? Alot of energy is wasted and one obtains a uniform measure (i.e. maximal entropy).\nExamples of subbit and coarse bits are provided by images of deterministic maps/reductions \\(X \\mapsto Z_1\\) and \\(Y_1 \\mapsto Z_2\\). This is what happens when a customer is asked to provide some “personal information”.\nWhat is the answer to the under/over problem? Can we predict whether we are before or after the inflection? Is it worth retraining, can we have any expectation of performance improvement? This would depend on the current dimension of the correlation.\n[-to be contined, JHM]"
  },
  {
    "objectID": "posts/2024-06-05-SteklovTrace/index.html",
    "href": "posts/2024-06-05-SteklovTrace/index.html",
    "title": "Idea On Steklov-Selberg Trace Formula for Arithmetic Spaces",
    "section": "",
    "text": "Here is a simple idea.\nThe origin of Langlands’ program is somewhere in trying to correlate the spectral data of Laplace operators on noncompact arithmetic spaces \\(\\Gamma \\backslash X\\) analogous to \\(GL({\\bf{Z}}^2) \\backslash PGL({\\bf{R}}^2)\\) with spectra of various “dual” spaces. Compare J. Arthurs’ comment on Langlands and relative trace formulas (Arthur 2005, 2007). The dominant object is the spectrum of the Laplacian \\(\\Delta\\) on the noncompact symmetric space \\(\\Gamma \\backslash X\\). Historically, the continuous part of the spectrum was discovered via Eisenstein series. Then the “discrete part” of the spectrum was defined by somehow “subtracting” the continuous part. Yet we find this procedure unconvincing. In a certain physical sense, a noncompact open space does not have any resonant frequencies because there is no boundary or reflection from which waves can “bounce back”. I.e. the noncompact space has no periodicity and no resonant frequencies.\nThe basic idea in our development is the maximal \\(\\bf{Q}\\)-rational Borel-Serre bordification \\(X[t]\\) of \\(X\\). See (Borel and Serre 1973). Our construction of \\(X[t]\\) via excisions of rational horoballs at-infinity implies \\(X[t] \\subset X\\) has a \\(\\Gamma\\)-equivariant proper discontinuous boundary \\(Y := \\partial X[t]\\). The key property of the rational bordification is that the reduced integral homology of \\(Y\\) when considered as a \\(\\bf{Z} \\Gamma\\)-module is the Bieri-Eckmann homological dualizing module.\nOur main proposal is to replace the spectrum of the Laplacian on noncompact \\(\\Gamma \\backslash X\\) with the spectrum of the Steklov operator on the bordification \\(\\Gamma \\backslash X[t]\\). In this way all the technical difficulties of trying to separate the continuous and discrete parts of the spectra are avoided, and we have as foundation the discrete Steklov spectrum on \\(X[t]\\).\nThis idea naturally suggests a Steklov-Selberg trace formula on \\(X[t]\\) analogous to (Selberg 1956).\nClaim: There exists a Poisson kernel \\(P(x,y)\\) and Poisson integral formula describing all harmonic extensions \\(\\hat{f}\\) of bounded functions \\(f\\) defined on \\(Y:=\\partial X[t]\\). The Steklov operator, also called the Dirichlet-to-Neumann map in the literature, is obtained by differentiation, and we have \\[Sf(x):=\\int_Y f(y) \\frac{\\partial P}{\\partial \\nu}(x,y) dy\\] for all \\(x\\in X[t]\\).\nClaim: The Steklov operator \\(S\\) has discrete countable spectrum and the spectral trace defined by the sum of eigenvalues \\(\\sum \\lambda\\) converges absolutely.\nClaim: The kernel \\(k(x,y):= \\frac{\\partial P}{\\partial \\nu}(x,y)\\) is absolutely integrable modulo \\(\\Gamma\\) and the geometric trace defined by integration along the diagonal \\(\\int_Y k(y,y) dy\\) converges absolutely modulo \\(\\Gamma\\).\nClaim: The spectral trace is equal to the geometric trace, and the geometric trace can be decomposed into a recursive sum of orbital-type integrals over conjugacy classes following Selberg.\n[To Be Continued - JHM]\n\n\n\n\nReferences\n\nArthur, James. 2005. “An Introduction to the Trace Formula.” Harmonic Analysis, the Trace Formula, and Shimura Varieties 4: 1–263.\n\n\n———. 2007. “A (Very Brief) History of the Trace Formula.” Newslett. Pac. Inst. Math. Sci. 10: 8–11.\n\n\nBorel, Armand, and Jean-Pierre Serre. 1973. “Corners and Arithmetic Groups.” Comment. Math. Helv 48 (1): 436–91.\n\n\nSelberg, Atle. 1956. “Harmonic Analysis and Discontinuous Groups in Weakly Symmetric Spaces with Applications to Dirichlet Series.” J. Indian Math. Soc. 20: 47–87."
  },
  {
    "objectID": "posts/2024-12-18-EffectiveComputeFrontier/index.html",
    "href": "posts/2024-12-18-EffectiveComputeFrontier/index.html",
    "title": "On the Efficient Computing Frontier",
    "section": "",
    "text": "On the Efficient Computing Frontier\nOur work on \\(P \\neq NP\\) and the Tao-Szemeredi Regularity Lemma provides a simple explanation of “why AI can’t cross this line”, the so-called Efficient Compute Frontier as described here (\"A.i. Can’t Cross This Line and We Don’t Know Why\" n.d.).\n\n\n\n“AI Can’t Cross This Line and We Don’t Know Why”\n\n\nOur explanation is simply this: the line can’t be crossed because there is an inherent ontological gap between representation and reality. In simple terms, images of cats don’t meow. A cat is a cat by nature of its physical qualities, and is only determined to a degree by the qualities of images of cats. This is trivial and philosophical, but is the fundamental reason why “AI Can’t Cross This Line”. Nothing can cross this line between the line is not separated by a distance, but by nature! The line is the inherent inescapable gap between numerical representation and reality. A representation will always have a maximal capacity for statistically significant (SS) information relevant to predicting an objective real world event. The limitation is a measure of the ontological gap and is independant of computing power and number of parameters used in the representation.\nEvery ML model is based on the assumption that a given data representation is statistically significant (SS) to predicting and correlating some objective real world events (Chollet 2021). This assumption of statistical significance is more or less reasonable in most cases.\nE.g. MNIST classification reasonably assumes that pixel values of images of handwritten letters are SS in predicting the represented letter. This is somewhat tautological, since we handwrite letters precisely with the intention of specifically representing the letter. In contrast, training MNIST classification based on the “sound” of pens used when writing the letters, or based on the length (amount of ink) used in drawing out the letters would not be SS.\nE.g. Training ML models to classify images of “cats” versus “dogs” assumes that pixel values of such images are SS in predicting the species. But training ML models to classify “cats” versus “dogs” based on images of their fur, or images of their pawprints are less SS in predicting the species. We would never expect these fur or pawprint models models to achieve near 100% accuracy regardless of how much training or computing power is invested.\nPerhaps these examples seem trivial, yet the point of the effective compute frontier is that every representation eventually reaches this level of absurdity! Formally this is expressed in Tao-Szemeredi’s Regularity Lemma (Tao 2005). The point of Tao-Szemeredi is that every representation has a ceiling to how much useful SS information it contains. The limitation is ontological and independant of computing power. A representation is only a representation, and there will always be an ontological gap between the representation and the objective reality.\nThe representation has a maximal capacity which contains the SS information, and beyond this capacity the representation is totally statistically insignificant. Tao-Szemeredi formalizes this idea that any given representation contains only so many significant bits. Conditioning on these most significant bits, everything else is random and uncorrelated! In detecting cats vs dogs via jpg images, there is only so much information contained in the image bits which is significant to the basic question of labelling “cats” and “dogs”. It happens that cats and dogs are often distinguished by appearance. But this is coincidental. And indeed cat-dog classifiers require images of their faces, and cannot readily classify the animals based on images of their legs or pawprints or ears, for example. Other species look similar, and are not readily distinguished by images. For example trees are not easily distinguished by images of their bark surfaces. Again these cases seem obvious, but the point of Tao-Szemeredi is that every representation eventually reaches this same level of absurdity. Do we really expect images of cats and dogs to contain perfect information significant to deciding them apart?\nTao-Szemeredi predicts a gap between how much significant information is contained in any given representation. This implies an a priori maximum accuracy of any model. Depending on the underlying objects, the representation is more or less efficient. An image of a tomato only contains so much information relevant to whether the tomato is ripe. Certainly there is some significant information, but the limit of SS information is a measure of the ontological gap between tomatos the vegetable and images of tomatoes. The ontological gap is independant of the number of parameters and the available compute power.\n\n\nInherent Entropy Uncertainty\nOur explanation predicts that there are no asymptotical neural scaling laws, for this would predict that the EFC asymptotically reaches zero. No, we predict that the EFC asymptotically reaches the inherent entropy uncertainty of the representation model. Eventually every representation reaches a point of absurdity. We explain the EFC as the quantitative measure of the absurdity that images of cats can replace cats which meow.\nThe art of ML is to find those most significant bits in the most significant representations. I.e. we don’t predict cats versus dogs based on the sound their paws make while walking. There is certainly naivete evident in most data scientists who are indifferent to the representation and don’t recognize this inherent ontological limitations.\n\n\n\n\n\nReferences\n\n\"A.i. Can’t Cross This Line and We Don’t Know Why\". n.d. Accessed December 18, 2024.\n\n\nChollet, Francois. 2021. Deep Learning with Python. Simon; Schuster.\n\n\nTao, Terence. 2005. “Szemeredi’s Regularity Lemma Revisited.” arXiv Preprint Math/0504472."
  }
]